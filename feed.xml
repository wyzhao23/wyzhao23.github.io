<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://wyzhao23.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wyzhao23.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-31T04:54:05+00:00</updated><id>https://wyzhao23.github.io/feed.xml</id><subtitle>Personal website of Wenyuan (Warren) Zhao. </subtitle><entry><title type="html">Bayesian Deep Learning as Deep Gaussian Processes</title><link href="https://wyzhao23.github.io/blog/2025/deep-bayesian-learning/" rel="alternate" type="text/html" title="Bayesian Deep Learning as Deep Gaussian Processes"/><published>2025-01-21T11:46:00+00:00</published><updated>2025-01-21T11:46:00+00:00</updated><id>https://wyzhao23.github.io/blog/2025/deep-bayesian-learning</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2025/deep-bayesian-learning/"><![CDATA[<h2 id="deep-additive-kernel-learning">Deep Additive Kernel Learning</h2> <p>With the strengths of both deep learning and kernel methods like Gaussian Processes (GPs), Deep Kernel Learning (DKL) has gained considerable attention in recent years. From the computational perspective, however, DKL becomes challenging when the input dimension of the last-layer GP is high. To address this challenge, we propose the Deep Additive Kernel (DAK) model, which incorporates i) an additive structure for the last-layer GP; and ii) induced prior approximation for each GP component. This naturally leads to a last-layer Bayesian neural network (BNN) architecture. The proposed method enjoys the interpretability of DKL as well as the computational advantages of BNN. Empirical results show that the proposed approach outperforms state-of-the-art DKL methods in both regression and classification tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dak-480.webp 480w,/assets/img/dak-800.webp 800w,/assets/img/dak-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dak.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Check our latest paper:</p> <ul> <li>AISTATS 2025: From deep additive kernel learning to last-layer Bayesian neural networks via induced prior approximation</li> </ul> <h2 id="deep-markov-gaussian-processes">Deep Markov Gaussian Processes</h2> <p>Deep Gaussian Processes (DGPs) are a powerful class of probabilistic models that extend traditional GPs by stacking multiple GP layers. We develop software based on a class of DGPs with sparse structure, referred to as Deep Markov GPs (DMGPs). More specifically, we implement DMGPs as sparsely activated Bayesian Neural Networks (BNNs) with learnable weights and biases, which can be used in a wide range of deep learning applications. The depth of DGPs enables them to model intricate dependencies and variations, making them suitable for tasks such as regression, classification, and time-series analysis. The DMGP software can provide not only uncertainty quantification for predictions, but also interpretability by incorporating the neural additive structure. Our evaluation across various tabular datasets and visual object tasks shows that DMGPs not only match but often surpasses the performance of traditional DGP methods, all while significantly reducing the number of parameters. Due to the strong connection to deep neural networks, DMGPs can be easily extended to other state-of-art deep learning architectures and applications.</p> <p>DMGP package is available at https://github.com/warrenzha/dmgp.</p> <ul> <li><a href="https://dmgp.readthedocs.io">Documentation, examples, tutorials</a> on how to construct all sorts of DMGP models.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tmgp-480.webp 480w,/assets/img/tmgp-800.webp 800w,/assets/img/tmgp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/tmgp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[Deep Neural Networks (DNNs) are powerful tools capable of capturing intricate patterns in large datasets, but lack the ability to provide uncertainty estimation. To learn rich, hierarchical representations from data with proper interpretability and uncertainty estimation, efficient Bayesian deep learning gain considerable attention in a variety of real-world tasks, especially when uncertainty quantification is critical.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://wyzhao23.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://wyzhao23.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Private Information Retrieval</title><link href="https://wyzhao23.github.io/blog/2024/weakly-pir/" rel="alternate" type="text/html" title="Private Information Retrieval"/><published>2024-01-15T11:46:00+00:00</published><updated>2024-01-15T11:46:00+00:00</updated><id>https://wyzhao23.github.io/blog/2024/weakly-pir</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2024/weakly-pir/"><![CDATA[<h2 id="leaky-private-information-retrieval">Leaky Private Information Retrieval</h2> <p>We study the problem of leaky private information retrieval (L-PIR), where the amount of privacy leakage is measured by the pure differential privacy parameter, referred to as the leakage ratio exponent. Unlike the previous L-PIR scheme proposed by Samy et al., which only adjusted the probability allocation to the clean (low-cost) retrieval pattern, we optimize the probabilities assigned to all the retrieval patterns jointly. It is demonstrated that the optimal retrieval pattern probability distribution is quite sophisticated and has a layered structure: the retrieval patterns associated with the random key values of lower Hamming weights should be assigned higher probabilities. This new scheme provides a significant improvement, leading to an O(log K) leakage ratio exponent with fixed download cost D and number of servers N, in contrast to the previous art that only achieves a Θ(K) exponent, where K is the number of messages.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wpir-480.webp 480w,/assets/img/wpir-800.webp 800w,/assets/img/wpir-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wpir.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pirdp-480.webp 480w,/assets/img/pirdp-800.webp 800w,/assets/img/pirdp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pirdp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="weakly-private-information-retrieval">Weakly Private Information Retrieval</h2> <p>Private information retrieval (PIR) systems were motivated by the necessity to safeguard user privacy during information retrieval. In the standard PIR framework, a user wishes to retrieve a desired message from N non-colluding servers efficiently, such that the identity of the desired message is not leaked in a significant manner. We study the problem of weakly PIR when there is heterogeneity in servers’ trustfulness, i.e. some servers can be more trustworthy than others, under the maximal leakage (Max-L) metric and mutual information (MI) metric. A code construction is proposed for this setting and optimized the probability distribution for this construction.</p> <p>Check our latest paper:</p> <ul> <li><strong>ISIT 2024</strong>: <a href="https://arxiv.org/pdf/2402.17940">Weakly Private Information Retrieval from Heterogeneously Trusted Servers</a></li> </ul>]]></content><author><name></name></author><category term="Information-Theory"/><summary type="html"><![CDATA[Private information retrieval (PIR) systems are motivated by the necessity to safeguard user privacy during information retrieval. In the canonical PIR framework, a user wishes to retrieve a message from N independent servers, each holding a complete set of K messages. The message’s identity must remain hidden from any individual server. This privacy requirement necessarily incurs higher download costs than a protocol without such a requirement, which requires effort to improve the code construction of PIR systems.]]></summary></entry><entry><title type="html">AI-driven Dynamic mmWave Networking</title><link href="https://wyzhao23.github.io/blog/2023/mmwave-networking/" rel="alternate" type="text/html" title="AI-driven Dynamic mmWave Networking"/><published>2023-06-15T11:46:00+00:00</published><updated>2023-06-15T11:46:00+00:00</updated><id>https://wyzhao23.github.io/blog/2023/mmwave-networking</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2023/mmwave-networking/"><![CDATA[<h2 id="dynamic-mmwave-mesh-network">Dynamic mmWave Mesh Network</h2> <p>Millimeter-wavelength (mmWave) mesh network can provide multi-Gbps transmission but with large path loss and heterogeneous objectives which is hard to solve by heuristic models. Machine learning (ML) techniques, especially reinforcement learning (RL), have great potential in solving multi-objective, non-linear, and non-convex problems that often happen in mmWave mesh network configuration. On the other hand, network configuration policies learned from simulations cannot always help physical networks meet performance requirements due to sim2real gap. In this work, we develop a reinforcement learning (RL) model to train a policy for dynamic topology management and a self-supervised policy adaptation algorithm to bridge the domain gap. The experimental results shows that our RL agent can learn a policy to avoid blockage links and self-supervised learning model can help to eliminate domain gaps. The testbed we built can establish multiple routes and can be controlled effectively by a central controller. We successfully ran the simulation-trained RL policy and self-supervision agent on the real testbed.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dymesh-480.webp 480w,/assets/img/dymesh-800.webp 800w,/assets/img/dymesh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dymesh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="Communications"/><summary type="html"><![CDATA[Integrated Access and Backhaul (IAB) is an emerging technique to enable cost-effective deployment of dense 5G networks that utilize emerging millimeter-wavelength (mmWave) spectrum. Existing heuristic-based network control/management frameworks are not well-suited for the increasing complexity and uncertainty introduced by mmWave IAB. Machine learning (ML) can help automate network control decisions, but its practical deployment faces new system-level challenges in 5G IAB, including accurate simulation-based training, resolving conflicting objectives from heterogeneous network slices, and efficiently collecting observations for run-time decision-making.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://wyzhao23.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://wyzhao23.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Machine Learning and Data Science</title><link href="https://wyzhao23.github.io/blog/2021/machine-learning/" rel="alternate" type="text/html" title="Machine Learning and Data Science"/><published>2021-06-15T11:46:00+00:00</published><updated>2021-06-15T11:46:00+00:00</updated><id>https://wyzhao23.github.io/blog/2021/machine-learning</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2021/machine-learning/"><![CDATA[<h2 id="ml-based-matrix-optimization-in-massive-mimo">ML-based Matrix Optimization in Massive MIMO</h2> <p>In the downlink of massive MIMO, the transmitter uses precoding technology to reduce interference and improve spectrum efficiency. A complex-valued gradient neural network (CVGNN) is proposed to solve the Moore-Penrose inverse of the complex matrix in massive MIMO precoding algorithms. Theoretical linear convergence and numerical results are provided to corroborate the application of CVGNN in wireless communication senarios.</p> <h2 id="fog-computing-in-internet-of-vehicles">Fog Computing in Internet of Vehicles</h2> <p>As the technology of network, wireless transmission and big data computing develops rapidly, the society has entered the Information Era. Internet of Vehicles, the important part of the Internet of Things, is the inevitable trend of urban traffic in the future. Fog computing emerges as a new technology of distributed computing, which is suitable for applications in the Internet of Vehicles. Internet of Vehicles based on fog computing can solve the problems of traffic congestion, transportation efficiency and security. In this paper, we review the recent work in this topic and analyze the architecture and scenario of fog computing in the Internet of Vehicles. Three aspects of applications are proposed in this paper: VANET, big data processing and security. At the same time, we discuss the technical details and challenges in these applications. Fog computing has excellent processing power in real-time big data and a high mobility environment, which reduces the latency of data processing, makes the deployment between vehicles and roadside units more reasonable and enhances the security of information interchanged in the Internet of Vehicles. Fog computing devices will become an indispensable part of the future construction of Internet of Vehicles.</p> <p>“A Survey on Fog Computing Applications in Internet of Vehicles”, International Conference on Computing and Data Science, 2021.</p> <h2 id="online-review-classifier">Online Review Classifier</h2> <p>Customer reviews on e-commerce platforms contain valuable information, while sifting through them manually tends to dismay people because of the huge amount of data. This study implemented a machine learning-based algorithm to classify customer reviews. Our classifier extracts Chinese word segmentation and text frequency for feature extraction and scoring, and implements the classification with methods of Naive Bayesian and Support Vector Machines. Experimental results on the Taobao product review sentiment datasets show that our model based on two machine learning algorithms, though results in different performances, can provide suggestions on the selection of the identification classifier using a trade-off strategy and helps obtain fast and accurate classification on reviews of different categories.</p> <p>“<a href="https://iopscience.iop.org/article/10.1088/1742-6596/1678/1/012081/pdf">Classification of Customer Reviews on E-commerce Platforms Based on Naive Bayesian Algorithm and Support Vector Machine</a>”</p>]]></content><author><name></name></author><category term="Machine-Learning"/><summary type="html"><![CDATA[We studied the general area of machine learning and data science. A machine learning-based algorithm is proposed to classify customer reviews. A complex-valued gradient neural network (CVGNN) is proposed to solve the Moore-Penrose inverse of complex matrices. We also investigated fog computing in the Internet of Vehicles.]]></summary></entry><entry><title type="html">mmWave Beamforming</title><link href="https://wyzhao23.github.io/blog/2020/beamforming/" rel="alternate" type="text/html" title="mmWave Beamforming"/><published>2020-06-15T11:46:00+00:00</published><updated>2020-06-15T11:46:00+00:00</updated><id>https://wyzhao23.github.io/blog/2020/beamforming</id><content type="html" xml:base="https://wyzhao23.github.io/blog/2020/beamforming/"><![CDATA[<h2 id="beam-alignment-and-tracking">Beam Alignment and Tracking</h2> <p>Millimeter wave (mmwave) communications have attracted increasing attention thanks to the abundant spectrum resource. The short wave-length of mmwave signals facilitates exploiting large antenna arrays to achieve large array gains and combat large path-loss. However, the use of large antenna arrays along with narrow beams leads to a large overhead in beam training for obtaining channel state information, especially in dynamic environments. To reduce the overhead of beam training, in this paper we formulate the problem of beam alignment and tracking (BA/T) as a stochastic bandit problem. In particular, to sense the change of the environments, the actions are designed based on the offset of successive beam indexes (i.e., beam index difference), which measures the rate of change of the environments. Then, we propose two efficient BA/T algorithms based on the stochastic bandit learning. To reveal useful insights, the performance of effective achievable rate is further analyzed for the proposed BA/T algorithms. The analytical results show that the algorithms can sense the change of the environments and adjust beam training strategies intelligently. In addition, they do not require any priori knowledge of dynamic channel modeling, and thus are applicable to a variety of complicated scenarios. Simulation results demonstrate the effectiveness and superiority of the proposed algorithms.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bat-480.webp 480w,/assets/img/bat-800.webp 800w,/assets/img/bat-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/bat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="Communications"/><summary type="html"><![CDATA[The short wave-length of mmwave signals facilitates exploiting large antenna arrays to achieve large array gains and combat large path-loss. However, the use of large antenna arrays along with narrow beams leads to a large overhead in beam training for obtaining channel state information, especially in dynamic environments.]]></summary></entry></feed>